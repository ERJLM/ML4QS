{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Segmentation into Overlapping Windows"
      ],
      "metadata": {
        "id": "v-YQEXvoyeQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGVyEnsgyVVX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (adjust the file path if necessary, or skip this if df is already in memory)\n",
        "df = pd.read_csv('data/data_imputed.csv')\n",
        "print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "# Sort by time within each user group for proper temporal ordering\n",
        "df = df.sort_values(['user', 'seconds_elapsed']).reset_index(drop=True)\n",
        "\n",
        "# Define window parameters\n",
        "window_size = 200          # 2-second window (assuming 100 Hz data -> 200 samples)\n",
        "step_size = window_size // 2   # 50% overlap -> 100 samples shift\n",
        "\n",
        "# Segment into windows\n",
        "X_windows = []   # list to hold window feature arrays\n",
        "y_windows = []   # list to hold window labels\n",
        "for user_id, group in df.groupby('user'):\n",
        "    data = group.drop(columns=['user', 'seconds_elapsed']).values  # drop label and time, use feature values\n",
        "    n = len(data)\n",
        "    if n < window_size:\n",
        "        continue  # skip this user if not enough data for one window (unlikely here)\n",
        "    # Slide window with 50% overlap\n",
        "    for start in range(0, n - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        X_windows.append(data[start:end])\n",
        "        y_windows.append(user_id)\n",
        "\n",
        "# Convert lists to arrays\n",
        "X_windows = np.array(X_windows)\n",
        "y_windows = np.array(y_windows)\n",
        "print(\"Total windows formed:\", X_windows.shape[0])\n",
        "print(\"Window shape (timesteps x features):\", X_windows.shape[1], \"x\", X_windows.shape[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Train, Validation, and Test Sets"
      ],
      "metadata": {
        "id": "P1etvIK5y81m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# restore deprecated alias for legacy packages and python 3.8.8\n",
        "if not hasattr(np, \"int\"):\n",
        "    np.int = int        # or np.int64 if you prefer fixed width"
      ],
      "metadata": {
        "id": "A-gRcGw0z18S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the user labels (if they're not already numeric) to integers 0,1,2\n",
        "encoder = LabelEncoder()\n",
        "y_enc = encoder.fit_transform(y_windows)  # e.g. \"User_1\",\"User_2\",\"User_3\" -> 0,1,2\n",
        "\n",
        "# Stratified split into train+val and test\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_windows, y_enc, test_size=0.20, stratify=y_enc, random_state=42)\n",
        "# Further split train_val into actual train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
        "    # 0.25 of 0.8 is 0.2, so overall: 60% train, 20% val, 20% test\n",
        "\n",
        "# Print shapes and class distributions for verification\n",
        "print(\"Train set:\", X_train.shape, \"Validation set:\", X_val.shape, \"Test set:\", X_test.shape)\n",
        "print(\"Train class distribution:\", np.bincount(y_train))\n",
        "print(\"Test class distribution:\", np.bincount(y_test))"
      ],
      "metadata": {
        "id": "uUixot3VzCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline TCN Model"
      ],
      "metadata": {
        "id": "_RDIAEckzH22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tcn_model(input_shape, num_classes, num_filters=32, kernel_size=3, dilations=[1, 2, 4], dropout_rate=0.1):\n",
        "    \"\"\"\n",
        "    Builds a Temporal Convolutional Network model.\n",
        "    - input_shape: (timesteps, features)\n",
        "    - num_classes: number of output classes\n",
        "    - num_filters: number of convolutional filters in each conv layer\n",
        "    - kernel_size: size of the 1D convolution kernel\n",
        "    - dilations: list of dilation rates for each TCN block\n",
        "    - dropout_rate: dropout rate for SpatialDropout1D layers\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    # Optional initial projection to num_filters if input feature dimension is different\n",
        "    if num_filters != input_shape[-1]:\n",
        "        x = layers.Conv1D(num_filters, kernel_size=1, padding='same')(x)\n",
        "\n",
        "    # TCN residual blocks\n",
        "    for d in dilations:\n",
        "        residual = x  # save input for residual connection\n",
        "        # 1st convolution in the block\n",
        "        x = layers.Conv1D(num_filters, kernel_size, dilation_rate=d, padding='causal')(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.SpatialDropout1D(dropout_rate)(x)\n",
        "        # 2nd convolution in the block\n",
        "        x = layers.Conv1D(num_filters, kernel_size, dilation_rate=d, padding='causal')(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.SpatialDropout1D(dropout_rate)(x)\n",
        "        # Residual connection: make channel dims equal before adding, if needed\n",
        "        if residual.shape[-1] != x.shape[-1]:\n",
        "            residual = layers.Conv1D(num_filters, kernel_size=1, padding='same')(residual)\n",
        "        x = layers.Add()([x, residual])\n",
        "        x = layers.Activation('relu')(x)  # activation after adding residual\n",
        "\n",
        "    # Output layer: use the last time step's features for classification\n",
        "    # (Because of causal convolutions, the last time step contains info from the entire window)\n",
        "    last_step_feat = x[:, -1, :]              # tensor for last time-step features\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(last_step_feat)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build the model and show summary\n",
        "num_classes = len(encoder.classes_)\n",
        "tcn_model = build_tcn_model(input_shape=(window_size, X_train.shape[2]), num_classes=num_classes,\n",
        "                             num_filters=32, kernel_size=3, dilations=[1, 2, 4], dropout_rate=0.1)\n",
        "tcn_model.summary()"
      ],
      "metadata": {
        "id": "zZhOdZT5zDeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training with Early Stopping"
      ],
      "metadata": {
        "id": "xq5HeSVdzQHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "tcn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = tcn_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,             # maximum epochs (we expect early stopping to halt before this if converged)\n",
        "    batch_size=32,         # you can adjust batch size based on data size\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1             # verbose=1 to print training progress\n",
        ")"
      ],
      "metadata": {
        "id": "aa8SSIOmzVXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Metrics"
      ],
      "metadata": {
        "id": "RLg_LiEBzYoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Predict class probabilities on the test set\n",
        "y_prob = tcn_model.predict(X_test)\n",
        "y_pred = y_prob.argmax(axis=1)          # predicted class indices\n",
        "\n",
        "# Compute metrics\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "test_macro_auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Test Macro F1-score: {test_macro_f1:.3f}\")\n",
        "print(f\"Test Macro AUC: {test_macro_auc:.3f}\")"
      ],
      "metadata": {
        "id": "PteY60xFzgVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Uv5-WV2Azhtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner -q\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Define a model-building function for KerasTuner\n",
        "def build_model(hp):\n",
        "    # Hyperparameters to tune:\n",
        "    filters = hp.Int('filters', min_value=16, max_value=64, step=16)            # e.g. 16, 32, 48, 64\n",
        "    kernel = hp.Choice('kernel_size', values=[3, 5])                            # e.g. 3 or 5\n",
        "    dropout = hp.Choice('dropout_rate', values=[0.0, 0.1, 0.2])\n",
        "    # You could also tune number of dilation blocks, but for simplicity keep it fixed at [1,2,4]\n",
        "\n",
        "    # Build TCN model with these hyperparameters\n",
        "    model = build_tcn_model(input_shape=(window_size, X_train.shape[2]), num_classes=num_classes,\n",
        "                             num_filters=filters, kernel_size=kernel, dilations=[1, 2, 4], dropout_rate=dropout)\n",
        "    # Compile model with a potentially tunable learning rate\n",
        "    lr = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Set up Hyperband tuner (you can also use RandomSearch or BayesianOptimization)\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='tuner_dir',\n",
        "    project_name='TCN_gait_tuning',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Run the hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             validation_data=(X_val, y_val),\n",
        "             epochs=20,\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
        "            )\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "aD9IUXZGzcJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}